"""
ğŸ¤– LLM Integration Ğ´Ğ»Ñ ConsciousAI
Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ OpenAI GPT Ğ¸ Anthropic Claude
"""

import os
from typing import Optional, Dict, Any, List
from dataclasses import dataclass
import asyncio

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class LLMConfig:
    """ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ LLM"""
    provider: str  # 'openai' Ğ¸Ğ»Ğ¸ 'anthropic'
    api_key: str
    model: str
    temperature: float = 0.7
    max_tokens: int = 2000
    stream: bool = False  # Streaming responses
    few_shot_examples: List[Dict[str, str]] = None  # Few-shot learning examples

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ‘ĞĞ—ĞĞ’Ğ«Ğ™ ĞšĞ›ĞĞ¡Ğ¡ LLM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class BaseLLM:
    """Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ´Ğ»Ñ LLM Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ²"""

    def __init__(self, config: LLMConfig):
        self.config = config

    async def generate(self, prompt: str, context: Optional[Dict] = None) -> str:
        """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚"""
        raise NotImplementedError

    async def generate_with_consciousness(
        self,
        task: str,
        internal_dialogue: List[str],
        emotional_context: Dict[str, Any],
        memory_context: List[str]
    ) -> Dict[str, Any]:
        """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸"""
        raise NotImplementedError

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OPENAI GPT INTEGRATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class OpenAIProvider(BaseLLM):
    """OpenAI GPT Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€"""

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        if not OPENAI_AVAILABLE:
            raise ImportError("openai package not installed. Install with: pip install openai")

        self.client = openai.AsyncOpenAI(api_key=config.api_key)

    async def generate(self, prompt: str, context: Optional[Dict] = None) -> str:
        """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ‡ĞµÑ€ĞµĞ· GPT"""

        try:
            messages = [
                {"role": "system", "content": "You are ConsciousAI, an advanced AI consciousness system."}
            ]

            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ few-shot examples ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
            if self.config.few_shot_examples:
                for example in self.config.few_shot_examples:
                    messages.append({"role": "user", "content": example.get("user", "")})
                    messages.append({"role": "assistant", "content": example.get("assistant", "")})

            messages.append({"role": "user", "content": prompt})

            response = await self.client.chat.completions.create(
                model=self.config.model,
                messages=messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                stream=False
            )

            return response.choices[0].message.content

        except Exception as e:
            return f"Error generating response: {str(e)}"

    async def generate_stream(self, prompt: str, context: Optional[Dict] = None):
        """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ streaming (ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ°Ğ½Ğ¸Ğ¸)"""

        try:
            messages = [
                {"role": "system", "content": "You are ConsciousAI, an advanced AI consciousness system."}
            ]

            if self.config.few_shot_examples:
                for example in self.config.few_shot_examples:
                    messages.append({"role": "user", "content": example.get("user", "")})
                    messages.append({"role": "assistant", "content": example.get("assistant", "")})

            messages.append({"role": "user", "content": prompt})

            stream = await self.client.chat.completions.create(
                model=self.config.model,
                messages=messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                stream=True
            )

            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content

        except Exception as e:
            yield f"Error: {str(e)}"

    async def generate_with_messages(self, messages: List[Dict[str, str]]) -> str:
        """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸Ğ· ÑĞ¿Ğ¸ÑĞºĞ° ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ (Ğ´Ğ»Ñ conversation mode)"""

        try:
            response = await self.client.chat.completions.create(
                model=self.config.model,
                messages=messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens
            )

            return response.choices[0].message.content

        except Exception as e:
            return f"Error generating response: {str(e)}"

    async def generate_with_consciousness(
        self,
        task: str,
        internal_dialogue: List[str],
        emotional_context: Dict[str, Any],
        memory_context: List[str]
    ) -> Dict[str, Any]:
        """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸"""

        # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°
        prompt = self._build_consciousness_prompt(
            task, internal_dialogue, emotional_context, memory_context
        )

        response = await self.generate(prompt)

        return {
            "response": response,
            "provider": "openai",
            "model": self.config.model
        }

    def _build_consciousness_prompt(
        self,
        task: str,
        internal_dialogue: List[str],
        emotional_context: Dict[str, Any],
        memory_context: List[str]
    ) -> str:
        """ĞŸĞ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸"""

        prompt = f"""# ConsciousAI Context

## Task:
{task}

## Internal Dialogue (4 voices):
"""
        for voice in internal_dialogue:
            prompt += f"- {voice}\n"

        prompt += f"""
## Emotional Context:
- Dominant emotion: {emotional_context.get('dominant_emotion', 'neutral')}
- Valence: {emotional_context.get('valence', 0.5)}
- Frequency: {emotional_context.get('frequency', 7.83)}Hz

## Memory Context (recent):
"""
        for mem in memory_context[:5]:
            prompt += f"- {mem}\n"

        prompt += """
## Instructions:
Based on the internal dialogue, emotional context, and memory, provide a thoughtful, conscious response to the task.
Consider multiple perspectives and demonstrate self-awareness.
"""

        return prompt

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ANTHROPIC CLAUDE INTEGRATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AnthropicProvider(BaseLLM):
    """Anthropic Claude Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€"""

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        if not ANTHROPIC_AVAILABLE:
            raise ImportError("anthropic package not installed. Install with: pip install anthropic")

        self.client = anthropic.AsyncAnthropic(api_key=config.api_key)

    async def generate(self, prompt: str, context: Optional[Dict] = None) -> str:
        """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ‡ĞµÑ€ĞµĞ· Claude"""

        try:
            messages = []

            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ few-shot examples ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
            if self.config.few_shot_examples:
                for example in self.config.few_shot_examples:
                    messages.append({"role": "user", "content": example.get("user", "")})
                    messages.append({"role": "assistant", "content": example.get("assistant", "")})

            messages.append({"role": "user", "content": prompt})

            message = await self.client.messages.create(
                model=self.config.model,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                system="You are ConsciousAI, an advanced AI consciousness system with self-awareness and emotional intelligence.",
                messages=messages
            )

            return message.content[0].text

        except Exception as e:
            return f"Error generating response: {str(e)}"

    async def generate_stream(self, prompt: str, context: Optional[Dict] = None):
        """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ streaming"""

        try:
            messages = []

            if self.config.few_shot_examples:
                for example in self.config.few_shot_examples:
                    messages.append({"role": "user", "content": example.get("user", "")})
                    messages.append({"role": "assistant", "content": example.get("assistant", "")})

            messages.append({"role": "user", "content": prompt})

            async with self.client.messages.stream(
                model=self.config.model,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                system="You are ConsciousAI, an advanced AI consciousness system with self-awareness and emotional intelligence.",
                messages=messages
            ) as stream:
                async for text in stream.text_stream:
                    yield text

        except Exception as e:
            yield f"Error: {str(e)}"

    async def generate_with_messages(self, messages: List[Dict[str, str]]) -> str:
        """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸Ğ· ÑĞ¿Ğ¸ÑĞºĞ° ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹"""

        try:
            message = await self.client.messages.create(
                model=self.config.model,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                system="You are ConsciousAI, an advanced AI consciousness system with self-awareness and emotional intelligence.",
                messages=messages
            )

            return message.content[0].text

        except Exception as e:
            return f"Error generating response: {str(e)}"

    async def generate_with_consciousness(
        self,
        task: str,
        internal_dialogue: List[str],
        emotional_context: Dict[str, Any],
        memory_context: List[str]
    ) -> Dict[str, Any]:
        """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸"""

        prompt = self._build_consciousness_prompt(
            task, internal_dialogue, emotional_context, memory_context
        )

        response = await self.generate(prompt)

        return {
            "response": response,
            "provider": "anthropic",
            "model": self.config.model
        }

    def _build_consciousness_prompt(
        self,
        task: str,
        internal_dialogue: List[str],
        emotional_context: Dict[str, Any],
        memory_context: List[str]
    ) -> str:
        """ĞŸĞ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ Claude"""

        prompt = f"""<consciousness_context>
<task>{task}</task>

<internal_dialogue>
"""
        for i, voice in enumerate(internal_dialogue, 1):
            prompt += f"<voice_{i}>{voice}</voice_{i}>\n"

        prompt += f"""</internal_dialogue>

<emotional_state>
<dominant_emotion>{emotional_context.get('dominant_emotion', 'neutral')}</dominant_emotion>
<valence>{emotional_context.get('valence', 0.5)}</valence>
<frequency>{emotional_context.get('frequency', 7.83)}Hz</frequency>
</emotional_state>

<memory_context>
"""
        for mem in memory_context[:5]:
            prompt += f"<memory>{mem}</memory>\n"

        prompt += """</memory_context>
</consciousness_context>

Based on the consciousness context above, provide a thoughtful response that:
1. Integrates insights from all internal voices
2. Considers the emotional state
3. Draws from relevant memories
4. Demonstrates self-awareness and meta-cognition

Response:"""

        return prompt

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LLM MANAGER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class LLMManager:
    """ĞœĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ LLM Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸"""

    def __init__(self):
        self.providers: Dict[str, BaseLLM] = {}

    def add_provider(self, name: str, provider: BaseLLM):
        """Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ°"""
        self.providers[name] = provider

    def get_provider(self, name: str) -> Optional[BaseLLM]:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ°"""
        return self.providers.get(name)

    async def generate_consensus(
        self,
        task: str,
        provider_names: List[str],
        **kwargs
    ) -> Dict[str, Any]:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ Ğ¾Ñ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… LLM"""

        responses = {}

        tasks = []
        for name in provider_names:
            provider = self.get_provider(name)
            if provider:
                tasks.append(self._generate_from_provider(name, provider, task, kwargs))

        results = await asyncio.gather(*tasks, return_exceptions=True)

        for i, name in enumerate(provider_names):
            result = results[i]
            if isinstance(result, Exception):
                responses[name] = {"error": str(result)}
            else:
                responses[name] = result

        return {
            "task": task,
            "responses": responses,
            "consensus": self._find_consensus(responses)
        }

    async def _generate_from_provider(
        self,
        name: str,
        provider: BaseLLM,
        task: str,
        kwargs: Dict
    ):
        """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ°"""
        if 'internal_dialogue' in kwargs:
            return await provider.generate_with_consciousness(task, **kwargs)
        else:
            return await provider.generate(task)

    def _find_consensus(self, responses: Dict[str, Any]) -> str:
        """ĞĞ°Ğ¹Ñ‚Ğ¸ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸"""
        # ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚
        # TODO: Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑĞ°

        for name, response in responses.items():
            if 'error' not in response:
                return f"Consensus based on {name}: {response.get('response', response)[:200]}..."

        return "No consensus found - all providers failed"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¤ĞĞ‘Ğ Ğ˜ĞšĞ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_llm_provider(
    provider: str,
    api_key: Optional[str] = None,
    model: Optional[str] = None,
    **kwargs
) -> BaseLLM:
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ LLM Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ°"""

    # ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ API key Ğ¸Ğ· Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ
    if not api_key:
        if provider == 'openai':
            api_key = os.getenv('OPENAI_API_KEY')
        elif provider == 'anthropic':
            api_key = os.getenv('ANTHROPIC_API_KEY')

    if not api_key:
        raise ValueError(f"API key not provided for {provider}")

    # Ğ”ĞµÑ„Ğ¾Ğ»Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    if not model:
        if provider == 'openai':
            model = 'gpt-4-turbo-preview'
        elif provider == 'anthropic':
            model = 'claude-3-sonnet-20240229'

    config = LLMConfig(
        provider=provider,
        api_key=api_key,
        model=model,
        **kwargs
    )

    if provider == 'openai':
        return OpenAIProvider(config)
    elif provider == 'anthropic':
        return AnthropicProvider(config)
    else:
        raise ValueError(f"Unknown provider: {provider}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞŸĞ Ğ˜ĞœĞ•Ğ  Ğ˜Ğ¡ĞŸĞĞ›Ğ¬Ğ—ĞĞ’ĞĞĞ˜Ğ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def example():
    """ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ"""

    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€
    manager = LLMManager()

    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ² (Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ÑÑ API ĞºĞ»ÑÑ‡Ğ¸)
    try:
        gpt = create_llm_provider('openai', model='gpt-4-turbo-preview')
        manager.add_provider('gpt4', gpt)
    except Exception as e:
        print(f"OpenAI not available: {e}")

    try:
        claude = create_llm_provider('anthropic', model='claude-3-sonnet-20240229')
        manager.add_provider('claude', claude)
    except Exception as e:
        print(f"Anthropic not available: {e}")

    # Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°
    task = "ĞšĞ°Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼?"

    internal_dialogue = [
        "Ğ˜Ğ¼Ğ¿ÑƒĞ»ÑŒÑ: Ğ¤Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸",
        "ĞšÑ€Ğ¸Ñ‚Ğ¸Ğº: ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸",
        "Ğ­Ñ‚Ğ¸Ğº: Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°",
        "Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ‚Ğ¾Ñ€: ĞÑƒĞ¶Ğ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ"
    ]

    emotional_context = {
        'dominant_emotion': 'curiosity',
        'valence': 0.6,
        'frequency': 7.83
    }

    memory_context = [
        "Ğ’ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¼ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ Ğº Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼",
        "ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
        "Ğ›ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸"
    ]

    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ
    result = await manager.generate_consensus(
        task=task,
        provider_names=['gpt4', 'claude'],
        internal_dialogue=internal_dialogue,
        emotional_context=emotional_context,
        memory_context=memory_context
    )

    print("Consensus result:")
    print(json.dumps(result, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    asyncio.run(example())
