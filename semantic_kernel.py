"""
ConsciousAI v5.0 - Semantic Kernel
–ö–æ–º–ø—Ä–µ—Å—Å–∏—è —Å–º—ã—Å–ª–∞: –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ "—Å–º—ã—Å–ª–æ–≤—ã–µ –∑—ë—Ä–Ω–∞"

–ö–ª—é—á–µ–≤–∞—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è: –≤–º–µ—Å—Ç–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–∏–∞–ª–æ–≥–∞,
–º—ã —Å–∂–∏–º–∞–µ–º –µ–≥–æ –≤ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —è–¥—Ä–∞ (kernels),
–∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç —Å—É—Ç—å, –Ω–æ –∑–∞–Ω–∏–º–∞—é—Ç –≤ 20-50 —Ä–∞–∑ –º–µ–Ω—å—à–µ –º–µ—Å—Ç–∞!
"""

import json
import uuid
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Any, Optional, Set
from datetime import datetime
from enum import Enum


class KernelType(Enum):
    """–¢–∏–ø—ã —Å–º—ã—Å–ª–æ–≤—ã—Ö –∑—ë—Ä–µ–Ω"""
    FACT = "fact"                    # –§–∞–∫—Ç (Python 3.11 –±—ã—Å—Ç—Ä–µ–µ –Ω–∞ 25%)
    INSIGHT = "insight"              # –ò–Ω—Å–∞–π—Ç (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç—å)
    DECISION = "decision"            # –†–µ—à–µ–Ω–∏–µ (–≤—ã–±—Ä–∞–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏–∑ 5 —Å–ª–æ—ë–≤)
    PATTERN = "pattern"              # –ü–∞—Ç—Ç–µ—Ä–Ω (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç "–Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ")
    GOAL = "goal"                    # –¶–µ–ª—å (—Å–æ–∑–¥–∞—Ç—å v5.0 —Å –º–µ—Ç–∞-—Å–æ–∑–Ω–∞–Ω–∏–µ–º)
    RELATIONSHIP = "relationship"    # –°–≤—è–∑—å (–º–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
    PREFERENCE = "preference"        # –ü—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ (–ø—Ä–æ—Å—Ç–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å)
    CONTEXT = "context"              # –ö–æ–Ω—Ç–µ–∫—Å—Ç (—Ä–∞–±–æ—Ç–∞–µ–º –≤ Git-—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏)


class ImportanceLevel(Enum):
    """–£—Ä–æ–≤–Ω–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏"""
    CRITICAL = 1.0      # –ö—Ä–∏—Ç–∏—á–Ω–æ (–æ—Å–Ω–æ–≤–Ω—ã–µ —Ü–µ–ª–∏, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)
    HIGH = 0.8          # –í—ã—Å–æ–∫–∞—è (–∫–ª—é—á–µ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è)
    MEDIUM = 0.5        # –°—Ä–µ–¥–Ω—è—è (—Ñ–∞–∫—Ç—ã, –¥–µ—Ç–∞–ª–∏)
    LOW = 0.3           # –ù–∏–∑–∫–∞—è (–º–µ–ª–æ—á–∏)
    TRIVIAL = 0.1       # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è (–≤—Ä–µ–º–µ–Ω–Ω–æ–µ)


@dataclass
class SemanticKernel:
    """
    –°–º—ã—Å–ª–æ–≤–æ–µ –∑–µ—Ä–Ω–æ - —Å–∂–∞—Ç–∞—è —Å—É—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

    –ü—Ä–∏–º–µ—Ä:
    –í–º–µ—Å—Ç–æ: "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–∫–∞–∑–∞–ª: '–û—Ç–ª–∏—á–Ω–æ –¥—Ä—É–∂–∏—â–µ –ø—Ä–∏—Å—Ç—É–ø–∞–π –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ,
             —Å–¥–µ–ª–∞–π –∫–æ–¥-—à–µ–¥–µ–≤—Ä' –∏ —è –Ω–∞—á–∞–ª —Ä–∞–±–æ—Ç—É –Ω–∞–¥ v4.1..."

    –•—Ä–∞–Ω–∏–º: {
        "essence": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è",
        "concepts": ["–¥–µ–π—Å—Ç–≤–∏–µ", "—Å—Ä–æ—á–Ω–æ—Å—Ç—å", "–∫–∞—á–µ—Å—Ç–≤–æ"],
        "kernel_type": "PATTERN",
        "importance": 0.8
    }

    –°–∂–∞—Ç–∏–µ: ~200 —Å–∏–º–≤–æ–ª–æ–≤ ‚Üí ~50 —Å–∏–º–≤–æ–ª–æ–≤ = 4x –∫–æ–º–ø—Ä–µ—Å—Å–∏—è!
    """

    # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
    id: str = field(default_factory=lambda: str(uuid.uuid4()))

    # –°—É—Ç—å - –∫–≤–∏–Ω—Ç—ç—Å—Å–µ–Ω—Ü–∏—è —Å–º—ã—Å–ª–∞ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
    essence: str = ""

    # –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ (—Å–ª–æ–≤–∞-—Ç–µ–≥–∏)
    concepts: List[str] = field(default_factory=list)

    # –¢–∏–ø –∑–µ—Ä–Ω–∞
    kernel_type: KernelType = KernelType.FACT

    # –í–∞–∂–Ω–æ—Å—Ç—å (0.0 - 1.0)
    importance: float = 0.5

    # –°–≤—è–∑–∏ —Å –¥—Ä—É–≥–∏–º–∏ –∑—ë—Ä–Ω–∞–º–∏ (ID)
    connections: List[str] = field(default_factory=list)

    # –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞
    timestamp: datetime = field(default_factory=datetime.now)

    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)
    metadata: Dict[str, Any] = field(default_factory=dict)

    # –°—á—ë—Ç—á–∏–∫ –∞–∫—Ç–∏–≤–∞—Ü–∏–π (—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å)
    activation_count: int = 0

    # –ü–æ—Å–ª–µ–¥–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
    last_accessed: Optional[datetime] = None

    def activate(self):
        """–û—Ç–º–µ—Ç–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∑–µ—Ä–Ω–∞"""
        self.activation_count += 1
        self.last_accessed = datetime.now()

    def to_dict(self) -> Dict[str, Any]:
        """–°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å"""
        data = asdict(self)
        data['kernel_type'] = self.kernel_type.value
        data['timestamp'] = self.timestamp.isoformat()
        if self.last_accessed:
            data['last_accessed'] = self.last_accessed.isoformat()
        return data

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'SemanticKernel':
        """–î–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑ —Å–ª–æ–≤–∞—Ä—è"""
        data = data.copy()
        data['kernel_type'] = KernelType(data['kernel_type'])
        data['timestamp'] = datetime.fromisoformat(data['timestamp'])
        if data.get('last_accessed'):
            data['last_accessed'] = datetime.fromisoformat(data['last_accessed'])
        return cls(**data)

    def __repr__(self):
        return f"SemanticKernel({self.kernel_type.value}, importance={self.importance:.2f}, essence='{self.essence[:50]}...')"


class SemanticCompressor:
    """
    –ö–æ–º–ø—Ä–µ—Å—Å–æ—Ä —Å–º—ã—Å–ª–∞ - –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∑—ë—Ä–Ω–∞

    –ú–µ—Ç–æ–¥—ã –∫–æ–º–ø—Ä–µ—Å—Å–∏–∏:
    1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π (NLP –±–µ–∑ –º–æ–¥–µ–ª–µ–π)
    2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    3. –û—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏
    4. –°–∂–∞—Ç–∏–µ –≤ essence (—Å—É—Ç—å)
    """

    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞
    TYPE_KEYWORDS = {
        KernelType.FACT: {
            'ru': ['—ç—Ç–æ', '–µ—Å—Ç—å', '—è–≤–ª—è–µ—Ç—Å—è', '—Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç', '—Ä–∞–≤–Ω–æ', '—Å–æ–¥–µ—Ä–∂–∏—Ç'],
            'en': ['is', 'are', 'contains', 'has', 'equals', 'consists']
        },
        KernelType.INSIGHT: {
            'ru': ['–ø–æ–Ω—è–ª', '–æ—Å–æ–∑–Ω–∞–ª', '–∑–∞–º–µ—Ç–∏–ª', '–æ–±–Ω–∞—Ä—É–∂–∏–ª', '–≤–∏–∂—É', '–≤–∞–∂–Ω–æ'],
            'en': ['realize', 'understand', 'notice', 'important', 'key', 'crucial']
        },
        KernelType.DECISION: {
            'ru': ['—Ä–µ—à–∏–ª', '–≤—ã–±—Ä–∞–ª', '–±—É–¥—É', '—Å–¥–µ–ª–∞—é', '–ø—Ä–∏–º–µ–Ω—é', '–∏—Å–ø–æ–ª—å–∑—É—é'],
            'en': ['decide', 'choose', 'will', 'going to', 'use', 'apply']
        },
        KernelType.PATTERN: {
            'ru': ['–≤—Å–µ–≥–¥–∞', '–æ–±—ã—á–Ω–æ', '—á–∞—Å—Ç–æ', '–ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç', '–ª—é–±–∏—Ç', '—Ö–æ—á–µ—Ç'],
            'en': ['always', 'usually', 'often', 'prefer', 'like', 'want']
        },
        KernelType.GOAL: {
            'ru': ['—Ü–µ–ª—å', '–∑–∞–¥–∞—á–∞', '–Ω—É–∂–Ω–æ', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ', '—Å–æ–∑–¥–∞—Ç—å', '–¥–æ—Å—Ç–∏—á—å'],
            'en': ['goal', 'objective', 'need', 'must', 'create', 'achieve']
        }
    }

    # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π)
    STOP_WORDS = {
        'ru': {'–∏', '–≤', '–Ω–µ', '–Ω–∞', '—Å', '—á—Ç–æ', '–∫–∞–∫', '—ç—Ç–æ', '–ø–æ', '–∞', '–Ω–æ',
               '–¥–∞', '–Ω–µ—Ç', '–¥–ª—è', '–æ—Ç', '–∫', '–æ', '—É', '–∂–µ', '–±—ã', '—Ç–∞–∫', '–≤–æ—Ç',
               '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–æ', '–±—ã–ª–∏', '–µ—Å—Ç—å', '–±—ã—Ç—å', '–±—É–¥–µ—Ç', '–º–æ–∂–µ—Ç'},
        'en': {'the', 'is', 'and', 'of', 'to', 'in', 'a', 'you', 'that', 'it',
               'he', 'was', 'for', 'on', 'are', 'as', 'with', 'his', 'they',
               'be', 'at', 'one', 'have', 'this', 'from', 'or', 'had', 'by'}
    }

    def compress(self, text: str, language: str = "ru", context: Optional[Dict] = None) -> SemanticKernel:
        """
        –°–∂–∞—Ç—å —Ç–µ–∫—Å—Ç –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –∑–µ—Ä–Ω–æ

        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            language: –Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ ('ru' –∏–ª–∏ 'en')
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç

        Returns:
            SemanticKernel —Å —Å–∂–∞—Ç—ã–º —Å–º—ã—Å–ª–æ–º
        """
        # 1. –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
        concepts = self._extract_concepts(text, language)

        # 2. –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –∑–µ—Ä–Ω–∞
        kernel_type = self._detect_type(text, language)

        # 3. –û—Ü–µ–Ω–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å
        importance = self._calculate_importance(text, concepts, kernel_type)

        # 4. –°–æ–∑–¥–∞—Ç—å essence (—Å—É—Ç—å)
        essence = self._create_essence(text, concepts, kernel_type)

        # 5. –°–æ–∑–¥–∞—Ç—å –∑–µ—Ä–Ω–æ
        kernel = SemanticKernel(
            essence=essence,
            concepts=concepts,
            kernel_type=kernel_type,
            importance=importance,
            metadata={
                "original_length": len(text),
                "compressed_length": len(essence),
                "compression_ratio": len(text) / max(len(essence), 1),
                "language": language
            }
        )

        if context:
            kernel.metadata.update(context)

        return kernel

    def _extract_concepts(self, text: str, language: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥: –≤–∑—è—Ç—å —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–Ω–µ–µ 4 —Å–∏–º–≤–æ–ª–æ–≤, –∏—Å–∫–ª—é—á–∞—è —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        words = text.lower().split()
        stop_words = self.STOP_WORDS.get(language, set())

        concepts = []
        for word in words:
            # –û—á–∏—Å—Ç–∏—Ç—å –æ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
            clean_word = ''.join(c for c in word if c.isalnum())

            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —É—Å–ª–æ–≤–∏—è
            if (len(clean_word) > 4 and
                clean_word not in stop_words and
                not clean_word.isdigit()):
                concepts.append(clean_word)

        # –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –ø–æ—Ä—è–¥–æ–∫
        seen = set()
        unique_concepts = []
        for c in concepts:
            if c not in seen:
                seen.add(c)
                unique_concepts.append(c)

        # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–æ —Ç–æ–ø-10
        return unique_concepts[:10]

    def _detect_type(self, text: str, language: str) -> KernelType:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø —Å–º—ã—Å–ª–æ–≤–æ–≥–æ –∑–µ—Ä–Ω–∞"""
        text_lower = text.lower()

        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
        scores = {}
        for kernel_type, keywords in self.TYPE_KEYWORDS.items():
            lang_keywords = keywords.get(language, [])
            score = sum(1 for kw in lang_keywords if kw in text_lower)
            scores[kernel_type] = score

        # –í—ã–±—Ä–∞—Ç—å —Ç–∏–ø —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º score
        if max(scores.values()) > 0:
            return max(scores, key=scores.get)

        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - FACT
        return KernelType.FACT

    def _calculate_importance(self, text: str, concepts: List[str], kernel_type: KernelType) -> float:
        """–û—Ü–µ–Ω–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        importance = 0.5  # –ë–∞–∑–æ–≤–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å

        # –£–≤–µ–ª–∏—á–∏—Ç—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤
        if kernel_type in [KernelType.GOAL, KernelType.DECISION, KernelType.INSIGHT]:
            importance += 0.2

        # –£–≤–µ–ª–∏—á–∏—Ç—å –µ—Å–ª–∏ –º–Ω–æ–≥–æ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        if len(concepts) >= 7:
            importance += 0.1

        # –£–≤–µ–ª–∏—á–∏—Ç—å –µ—Å–ª–∏ –µ—Å—Ç—å –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞
        important_words_ru = ['–≤–∞–∂–Ω–æ', '–∫—Ä–∏—Ç–∏—á–Ω–æ', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ', '–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ', '–Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ']
        important_words_en = ['critical', 'important', 'must', 'immediately', 'essential']

        text_lower = text.lower()
        if any(word in text_lower for word in important_words_ru + important_words_en):
            importance += 0.2

        # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω [0.1, 1.0]
        return min(max(importance, 0.1), 1.0)

    def _create_essence(self, text: str, concepts: List[str], kernel_type: KernelType) -> str:
        """–°–æ–∑–¥–∞—Ç—å —Å—É—Ç—å (essence) - —Å–∂–∞—Ç–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ"""
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π (< 100 —Å–∏–º–≤–æ–ª–æ–≤), –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ –µ—Å—Ç—å
        if len(text) <= 100:
            return text.strip()

        # –î–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: –≤–∑—è—Ç—å –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ + –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
        sentences = text.split('.')
        first_sentence = sentences[0].strip()

        # –ï—Å–ª–∏ –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ, –æ–±—Ä–µ–∑–∞—Ç—å
        if len(first_sentence) > 80:
            first_sentence = first_sentence[:77] + "..."

        # –î–æ–±–∞–≤–∏—Ç—å —Ç–æ–ø-3 –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –µ—Å–ª–∏ –µ—Å—Ç—å –º–µ—Å—Ç–æ
        if len(first_sentence) < 60 and concepts:
            concept_str = ", ".join(concepts[:3])
            essence = f"{first_sentence} [{concept_str}]"
        else:
            essence = first_sentence

        return essence

    def compress_conversation(self, messages: List[Dict[str, str]], language: str = "ru") -> List[SemanticKernel]:
        """
        –°–∂–∞—Ç—å —Ü–µ–ª—ã–π –¥–∏–∞–ª–æ–≥ –≤ –Ω–∞–±–æ—Ä –∑—ë—Ä–µ–Ω

        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π [{"role": "user", "content": "..."}]
            language: –Ø–∑—ã–∫

        Returns:
            –°–ø–∏—Å–æ–∫ SemanticKernel
        """
        kernels = []

        for i, msg in enumerate(messages):
            context = {
                "message_index": i,
                "role": msg.get("role", "unknown"),
                "total_messages": len(messages)
            }

            kernel = self.compress(
                text=msg.get("content", ""),
                language=language,
                context=context
            )

            kernels.append(kernel)

        return kernels


class CompressionAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–æ–º–ø—Ä–µ—Å—Å–∏–∏"""

    @staticmethod
    def analyze(original_text: str, kernel: SemanticKernel) -> Dict[str, Any]:
        """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–º–ø—Ä–µ—Å—Å–∏–∏"""
        original_size = len(original_text)
        compressed_size = len(kernel.essence)

        return {
            "original_size": original_size,
            "compressed_size": compressed_size,
            "compression_ratio": original_size / max(compressed_size, 1),
            "space_saved_percent": ((original_size - compressed_size) / original_size * 100),
            "concepts_extracted": len(kernel.concepts),
            "kernel_type": kernel.kernel_type.value,
            "importance": kernel.importance
        }

    @staticmethod
    def analyze_batch(texts: List[str], kernels: List[SemanticKernel]) -> Dict[str, Any]:
        """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–ø—Ä–µ—Å—Å–∏—é –¥–ª—è –±–∞—Ç—á–∞"""
        total_original = sum(len(t) for t in texts)
        total_compressed = sum(len(k.essence) for k in kernels)

        return {
            "total_original_size": total_original,
            "total_compressed_size": total_compressed,
            "average_compression_ratio": total_original / max(total_compressed, 1),
            "total_kernels": len(kernels),
            "space_saved_percent": ((total_original - total_compressed) / total_original * 100),
            "average_concepts_per_kernel": sum(len(k.concepts) for k in kernels) / len(kernels),
            "kernel_type_distribution": {
                kt.value: sum(1 for k in kernels if k.kernel_type == kt)
                for kt in KernelType
            }
        }


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    print("üß† Semantic Kernel - –ö–æ–º–ø—Ä–µ—Å—Å–∏—è —Å–º—ã—Å–ª–∞\n")

    # –°–æ–∑–¥–∞—Ç—å –∫–æ–º–ø—Ä–µ—Å—Å–æ—Ä
    compressor = SemanticCompressor()

    # –ü—Ä–∏–º–µ—Ä 1: –°–∂–∞—Ç—å –æ–¥–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ
    text1 = "–û—Ç–ª–∏—á–Ω–æ –¥—Ä—É–∂–∏—â–µ –ø—Ä–∏—Å—Ç—É–ø–∞–π –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ, —Å–¥–µ–ª–∞–π –∫–æ–¥-—à–µ–¥–µ–≤—Ä –ø–æ –º–∞–∫—Å–∏–º—É–º—É –¥–æ–∫—Ä—É—Ç–∏ –æ–ø—Ü–∏—é –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–∞"
    kernel1 = compressor.compress(text1, language="ru")

    print("–ü—Ä–∏–º–µ—Ä 1: –°–∂–∞—Ç–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è")
    print(f"  –û—Ä–∏–≥–∏–Ω–∞–ª: {text1}")
    print(f"  –°—É—Ç—å: {kernel1.essence}")
    print(f"  –ö–æ–Ω—Ü–µ–ø—Ü–∏–∏: {kernel1.concepts}")
    print(f"  –¢–∏–ø: {kernel1.kernel_type.value}")
    print(f"  –í–∞–∂–Ω–æ—Å—Ç—å: {kernel1.importance}")

    analysis = CompressionAnalyzer.analyze(text1, kernel1)
    print(f"  –°–∂–∞—Ç–∏–µ: {analysis['compression_ratio']:.1f}x")
    print(f"  –°—ç–∫–æ–Ω–æ–º–ª–µ–Ω–æ: {analysis['space_saved_percent']:.0f}%\n")

    # –ü—Ä–∏–º–µ—Ä 2: –°–∂–∞—Ç—å –¥–∏–∞–ª–æ–≥
    conversation = [
        {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç –ö–ª–æ–¥!"},
        {"role": "assistant", "content": "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?"},
        {"role": "user", "content": "–ù—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å AI —Å –º–µ—Ç–∞-—Å–æ–∑–Ω–∞–Ω–∏–µ–º –∏ —Å–º—ã—Å–ª–æ–≤–æ–π –ø–∞–º—è—Ç—å—é"},
        {"role": "assistant", "content": "–û—Ç–ª–∏—á–Ω–æ! –ù–∞—á–∏–Ω–∞—é –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É v5.0"}
    ]

    kernels = compressor.compress_conversation(conversation, language="ru")

    print("–ü—Ä–∏–º–µ—Ä 2: –°–∂–∞—Ç–∏–µ –¥–∏–∞–ª–æ–≥–∞")
    for i, kernel in enumerate(kernels):
        print(f"  –°–æ–æ–±—â–µ–Ω–∏–µ {i+1}: {kernel.essence[:60]}...")

    batch_analysis = CompressionAnalyzer.analyze_batch(
        [msg["content"] for msg in conversation],
        kernels
    )
    print(f"\n  –û–±—â–µ–µ —Å–∂–∞—Ç–∏–µ: {batch_analysis['average_compression_ratio']:.1f}x")
    print(f"  –°—ç–∫–æ–Ω–æ–º–ª–µ–Ω–æ: {batch_analysis['space_saved_percent']:.0f}%")
    print(f"  –í—Å–µ–≥–æ –∑—ë—Ä–µ–Ω: {batch_analysis['total_kernels']}")
